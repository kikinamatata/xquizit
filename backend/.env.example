# Google Gemini API Key
# Get your API key from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# DeepInfra API Key (OPTIONAL - Legacy, no longer required)
# WhisperLive is now used for real-time transcription instead of DeepInfra
# DEEPINFRA_API_KEY=your_deepinfra_api_key_here

# LLM Optimization Parameters
# Thinking budget: 0 = disabled, higher values allow more thinking time (increases latency)
GEMINI_THINKING_BUDGET=0

# Include thoughts: Set to 'true' to include LLM reasoning in responses (useful for debugging)
GEMINI_INCLUDE_THOUGHTS=false

# Max output tokens: Limit response size to reduce latency (default: 1024)
GEMINI_MAX_OUTPUT_TOKENS=1024

# Temperature: Controls randomness (0.0 = deterministic, 1.0 = more creative)
GEMINI_TEMPERATURE=0.7

# Interview Graph Architecture
# The system uses the V3 Hybrid Modular State Machine architecture exclusively:
#   - Conversational turn handling (detects clarifications, thinking, acknowledgments)
#   - Strategic time allocation (priority-based topic budgeting, no hard limits)
#   - Quality-driven follow-ups (unlimited, based on coverage + confidence)
#   - Intelligent topic selection (multi-factor scoring algorithm)

# ============================================================================
# TTS (TEXT-TO-SPEECH) CONFIGURATION
# ============================================================================

# TTS Backend Selection
# Choose which backend to use for text-to-speech audio generation:
#   - "kokoro": Use local RealtimeTTS with Kokoro engine (default, no external server required)
#   - "websocket": Use external WebSocket TTS server (requires separate TTS server running)
# Default: kokoro
TTS_BACKEND=kokor

# ----------------------------------------------------------------------------
# Kokoro TTS Configuration (used when TTS_BACKEND=kokoro)
# ----------------------------------------------------------------------------
# Device to run TTS model on ('cuda' for GPU, 'cpu' for CPU, 'auto' for automatic detection)
TTS_DEVICE=auto

# Kokoro voice to use (af_bella, af_sarah, am_adam, am_michael)
# af_bella: Adult Female - Bella (default, warm and professional)
# af_sarah: Adult Female - Sarah (clear and articulate)
# am_adam: Adult Male - Adam (deep and authoritative)
# am_michael: Adult Male - Michael (friendly and approachable)
KOKORO_VOICE=af_bella

# Speech speed multiplier (0.5 = slower, 1.0 = normal, 1.5 = faster)
# Range: 0.5-2.0, recommended: 1.0
KOKORO_SPEED=1.0

# ----------------------------------------------------------------------------
# WebSocket TTS Configuration (used when TTS_BACKEND=websocket)
# ----------------------------------------------------------------------------
# WebSocket TTS Server URL
# Connect to external TTS server via WebSocket for audio generation
# Examples:
#   - ws://localhost:8765 (local server, no SSL)
#   - ws://192.168.1.100:8765 (server on local network)
#   - wss://tts.example.com:443 (remote server with SSL)
# Default: ws://localhost:8765
TTS_SERVER_URL=ws://localhost:8765

# Uvicorn Auto-Reload (optional, set to 'true' to enable hot-reload during development)
# Note: Auto-reload can cause shutdown issues on Windows
UVICORN_RELOAD=false

# ============================================================================
# TRANSCRIPTION BACKEND CONFIGURATION
# ============================================================================

# Transcription Backend Selection
# Choose which backend to use for real-time audio transcription:
#   - "runpod": Use RunPod serverless transcription (cloud-based, HTTP API)
#   - "local": Use local WhisperLive server (faster, runs on local/network GPU)
# Default: runpod
TRANSCRIPTION_BACKEND=runpod

# ----------------------------------------------------------------------------
# RunPod Serverless Transcription (used when TRANSCRIPTION_BACKEND=runpod)
# ----------------------------------------------------------------------------
# Get your endpoint ID and API key from: https://runpod.io
# 1. Create account at https://runpod.io
# 2. Navigate to Serverless → Deploy a New Endpoint
# 3. Select "Whisper Faster" or "Faster Whisper" template
# 4. Copy your endpoint ID (format: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx)
# 5. Get API key from Settings → API Keys
RUNPOD_ENDPOINT_ID=your-runpod-endpoint-id-here
RUNPOD_API_KEY=your-runpod-api-key-here

# RunPod Transcription Settings (Optional)
# Maximum audio buffer size in seconds (default: 30)
RUNPOD_MAX_BUFFER_SECONDS=30

# ----------------------------------------------------------------------------
# Local WhisperLive Server (used when TRANSCRIPTION_BACKEND=local)
# ----------------------------------------------------------------------------
# Connect to WhisperLive server running on local network or same machine
# Server must be started separately (see: https://github.com/collabora/WhisperLive)
#
# Example: Start WhisperLive server on GPU system:
#   python -m whisper_live.server --port 9090 --backend faster_whisper --model large-v3
#
# Server Host: IP address or hostname of the system running WhisperLive server
# Examples: "localhost" (same machine), "192.168.1.100" (local network), "10.0.0.5"
WHISPERLIVE_SERVER_HOST=localhost

# Server Port: WhisperLive server port (default: 9090)
WHISPERLIVE_SERVER_PORT=9090

# Use SSL: Set to 'true' to use wss:// instead of ws:// (default: false)
# Only needed if your WhisperLive server is configured with SSL
WHISPERLIVE_USE_SSL=false

# ----------------------------------------------------------------------------
# Shared Transcription Settings (used by both runpod and local backends)
# ----------------------------------------------------------------------------
# Language code (e.g., en, es, fr) - default: en
WHISPERLIVE_LANGUAGE=en

# Whisper model size (small, medium, large-v3)
# small: Fast, good for development (less VRAM)
# large-v3: Production quality (better accuracy, more VRAM)
# Default: large-v3
WHISPERLIVE_MODEL=large-v3

# Voice Activity Detection (VAD) settings
# Enable VAD to filter silence (default: true)
WHISPERLIVE_USE_VAD=true

# No speech threshold: 0.0-1.0 (default: 0.45)
# Higher values = more aggressive silence filtering
WHISPERLIVE_NO_SPEECH_THRESH=0.45

# Same output threshold: Number of repeated outputs before finalizing a segment
# Used by both backends (default: 3 for local, 7 for RunPod)
WHISPERLIVE_SAME_OUTPUT_THRESHOLD=3

# Transcription interval: Seconds between API calls to RunPod (default: 3.0)
# Lower values = more frequent updates but higher API costs
# Higher values = less frequent updates but lower API costs
# Note: Only used with TRANSCRIPTION_BACKEND=runpod
WHISPERLIVE_TRANSCRIPTION_INTERVAL=3.0

# Transcription Logging (Optional)
# Set to 'false' to disable verbose transcription logs (default: true)
# Transcription logs can be very verbose and dominate the console output
# Disable this in production to keep logs clean and focused on application logic
TRANSCRIPTION_LOGGING_ENABLED=true

# ----------------------------------------------------------------------------
# Advanced Transcription Settings (local WhisperLive only)
# ----------------------------------------------------------------------------
# These settings provide fine-grained control over transcription behavior
# Only used when TRANSCRIPTION_BACKEND=local

# Send last N segments: Number of recent transcription segments to send for context (default: 10)
# Higher values = better context awareness but more data transfer
WHISPERLIVE_SEND_LAST_N_SEGMENTS=10

# Clip audio: Remove audio segments with no valid speech detection (default: false)
# Set to 'true' to reduce storage/processing of silent segments
WHISPERLIVE_CLIP_AUDIO=false

# Chunking mode: Audio chunking strategy (default: vad)
# Options:
#   - "vad": Voice Activity Detection-based chunking (adaptive, recommended)
#   - "time_based": Fixed-interval chunking (predictable timing)
WHISPERLIVE_CHUNKING_MODE=vad

# Chunk interval: Fixed interval for time-based chunking in seconds (default: 2.0)
# Only used when WHISPERLIVE_CHUNKING_MODE=time_based
# Range: 0.5-10.0, recommended: 2.0
WHISPERLIVE_CHUNK_INTERVAL=2.0

# ----------------------------------------------------------------------------
# Translation Settings (local WhisperLive only)
# ----------------------------------------------------------------------------
# Enable live translation alongside transcription
# Useful for multilingual interviews or cross-language communication
WHISPERLIVE_TARGET_LANGUAGE=en
# Enable translation: Set to 'true' to enable real-time translation (default: false)
WHISPERLIVE_ENABLE_TRANSLATION=false

# Target language: Language code for translation output (default: en)
# Examples: en (English), es (Spanish), fr (French), de (German), zh (Chinese)
# Note: Source language is detected automatically from audio

